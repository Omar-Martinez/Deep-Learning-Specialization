# [Improving Deep Neural Networks Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20Course)
The course focused on 
- Understanding the industry best-practices for building deep learning applications including initialization, L2 and dropout regularization, Batch normalization, gradient checking.
- Implementing and applying a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. - Understanding new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance 
- Impelementing a neural network in TensorFlow

## [Gradient Checking](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Gradient%20Checking/Gradient%20Checking%20v1.ipynb)
Intructions: "You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud--whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user's account has been taken over by a hacker. But backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, "Give me a proof that your backpropagation is actually working!" To give this reassurance, you are going to use "gradient checking".

## [Initialization](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Initialization/Initialization.ipynb)
Implemented zero, random, and He initializations

## [Regularization](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Regularization/Regularization_v2a.ipynb)
Used and compered different regularization techniques such as L2 regularization and dropout.

## [Optimization Methods](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week6/Optimization_methods_v1b.ipynb)
Learned and implemented advanced optimization methods that can speed up learning and perhaps get to a better final value for the cost function (Gradient descent, Mini-batch, Momentum,and Adam)

## [TensowFlow Tutorial](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week7/TensorFlow_Tutorial_v3b.ipynb)
Introducction to tensorflow and building aa neural network.
