# [Sequence Models Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/4.%20Sequence%20Models%20Course%20)
Learned how to build models for natural language, audio, and other sequence data. The course focused on:
- Understanding how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs. 
- Applying sequence models to natural language problems, including text synthesis. 
- Applying sequence models to audio applications, including speech recognition and music synthesis. 

## [Building your Recurrent Neural Network Step by Step](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step_v3b.ipynb)
Implemented key components of a Recurrent Neural Network in numpy.

## [Character level language model](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model_final_v3b.ipynb)
Built a model that generated new dinosaur names using a RNN model that could learn different name patterns from a previously collected dataset. The project focused on:
- How to store text data for processing using an RNN
- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit
- How to build a character-level text generation recurrent neural network
- Why clipping the gradients is important

## [Improvise a Jazz Solo with an LSTM Network](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v3a.ipynb)
Applied an LSTM to music generation and generate my own jazz music with deep learning.

## [Operations on word vectors](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb)
The project focused on:

- Loading pre-trained word vectors, and measure similarity using cosine similarity
- Using word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
- Modifying word embeddings to reduce their gender bias

## [Emojify](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%202/Emojify/Emojify_v2a.ipynb)
Used word vectors and an LSTM model to create an emojifier that could do the following:
- The model can turn the following sentence "Congratulations on the promotion! Let's get coffee and talk. Love you!" into "Congratulations on the promotion! üëç Let's get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è"

## [Neural Machine Translation](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb)
Built a Neural Machine Translation (NMT) model to translate human-readable dates ("25th of June, 2009") into machine-readable dates ("2009-06-25").These, using an attention model, one of the most sophisticated sequence-to-sequence models.

## [Trigger Word Detection](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%203/Trigger%20word%20detection/Trigger_word_detection_v1a.ipynb)
Learned about applying deep learning to speech recognition while constructing a speech dataset and implementing an algorithm for triggering word detection. By the end of this assignment, I was able to record a clip of myself talking, and have the algorithm trigger a chime when it detects the word "activate."

