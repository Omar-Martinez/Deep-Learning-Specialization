# [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=17-DeepLearning-LATAM&campaignid=6516520287&adgroupid=77982690923&device=c&keyword=coursera%20deep%20learning%20ai&matchtype=b&network=g&devicemodel=&adpostion=&creativeid=383456052668&hide_mobile_promo&gclid=EAIaIQobChMI8NvXiP-u6wIV1YtaBR1brgwLEAAYASAAEgKOZ_D_BwE)

The course was composed of five courses, were I learned the foundations of Deep Learning, understood how to build neural networks, and how to lead successful machine learning projects. I learned about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more, while working on on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. 

## 1. [Neural Networks & Deep Learning Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/1.%20Neural%20Networks%20%26%20Deep%20Learning%20Course)

Learned the foundations of deep learning, its architectures, how to implement it, and the major technology trends driving it through the following projects:

### [Logistic Regression with a Neural Network](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/1.%20Neural%20Networks%20%26%20Deep%20Learning%20Course/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb)
The first programminng assigment were I learned how to build the general architecture of a learning algorithm, including:
- Initializing parameters
- Calculating the cost function and its gradient
- Using an optimization algorithm (gradient descent)
- Gather all three functions above into a main model function, in the right order.

### [Planar Data_Classification with One Hidden Layer](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/1.%20Neural%20Networks%20%26%20Deep%20Learning%20Course/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb)
Built my first neural network with one hidden layer and learned how to:
- Implement a 2-class classification neural network with a single hidden layer
- Use units with a non-linear activation function, such as tanh
- Compute the cross entropy loss
- Implement forward and backward propagation

### [Building a Deep Neural Network Step by Step](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/1.%20Neural%20Networks%20%26%20Deep%20Learning%20Course/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb)
Built the functions for the implementation of a deep neural network with several hidden layers from scratch. 

### [Deep Neural Network for Image Classification: Application](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/1.%20Neural%20Networks%20%26%20Deep%20Learning%20Course/Week%204/Deep%20Neural%20Network%20Application:%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb)
Built a deep neural network for solving an image classification problem by using the functions that were created in the previous project.

## 2. [Improving Deep Neural Networks Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20Course)
The course focused on 
- Understanding the industry best-practices for building deep learning applications including initialization, L2 and dropout regularization, Batch normalization, gradient checking.
- Implementing and applying a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. - Understanding new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance 
- Impelementing a neural network in TensorFlow

### [Gradient Checking](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Gradient%20Checking/Gradient%20Checking%20v1.ipynb)
Intructions: "You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud--whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user's account has been taken over by a hacker. But backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, "Give me a proof that your backpropagation is actually working!" To give this reassurance, you are going to use "gradient checking".

### [Initialization](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Initialization/Initialization.ipynb)
Implemented zero, random, and He initializations

### [Regularization](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week5/Regularization/Regularization_v2a.ipynb)
Used and compered different regularization techniques such as L2 regularization and dropout.

### [Optimization Methods](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week6/Optimization_methods_v1b.ipynb)
Learned and implemented advanced optimization methods that can speed up learning and perhaps get to a better final value for the cost function (Gradient descent, Mini-batch, Momentum,and Adam)

### [TensowFlow Tutorial](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/2.%20Improving%20Deep%20Neural%20Networks%20Course/week7/TensorFlow_Tutorial_v3b.ipynb)
Introducction to tensorflow and building aa neural network.

## 3. Structuring Machine Learning Projects
The course provides "industry experience" that invidividuals might otherwise get only after years of ML work experience. It focuseses on
- Understanding how to diagnose errors in a machine learning system
- Prioritizing the most promising directions for reducing error 
- Understanding complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance
- How to apply end-to-end learning, transfer learning, and multi-task learning 

## 4. [Convolutional Neural Networks Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/3.%20Convolutional%20Neural%20Networks%20Course)
The course focused on:
- Understanding how to build a convolutional neural network, including recent variations such as residual networks. 
- How to apply convolutional networks to visual detection and recognition tasks.
- Using neural style transfer to generate art. 
- Apply these algorithms to a variety of image, video, and other 2D or 3D data. 

### [Convolutional Neural Networks: Step by Step](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week1/Convolution_model_Step_by_Step_v2a.ipynb)
Built and implemented convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation.

### [Emotion Detection in Images of Faces with Keras](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week2/KerasTutorial/Keras_Tutorial_v2a.ipynb)
Learned to use Keras, a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow and CNTK. As a final goal, created an app that classified emotions based on images.

### [Residual Networks](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week2/ResNets/Residual_Networks_v2a.ipynb)
Built the basic building blocks of ResNets (identity and convolutional blocks) and put them together to implement and train a state-of-the-art neural network for image classification.

### [Autonomous driving - Car detection](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week3/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection_v3a.ipynb)
Learned to use object detection on a car detection data set and dealing with bounding boxes. The project focused on the YOLO model and the ideas decribe inthe two YOLO papers: Redmon et al., 2016 and Redmon and Farhadi, 2016.

### [Face Recognition](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week4/Face%20Recognition/Face_Recognition_v3a.ipynb)
Built a face recognition system using many of the ideas coming from FaceNet (A neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person). These while:
- Implementing the triplet loss function
- Useing a pretrained model to map face images into 128-dimensional encodings
- Using these encodings to perform face verification and face recognition

### [Art Generation Style Transfer](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/3.%20Convolutional%20Neural%20Networks%20Course/week4/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer_v3a.ipynb)
Implemented the neural style transfer algorithm presented in Gatys et al. (2015) and generate novel artistic images .

## 4. [Sequence Models Course](https://github.com/Omar-Martinez/Deep-Learning-Specialization/tree/master/4.%20Sequence%20Models%20Course%20)
Learned how to build models for natural language, audio, and other sequence data. The course focused on:
- Understanding how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs. 
- Applying sequence models to natural language problems, including text synthesis. 
- Applying sequence models to audio applications, including speech recognition and music synthesis. 

### [Building your Recurrent Neural Network Step by Step](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step_v3b.ipynb)
Implemented key components of a Recurrent Neural Network in numpy.

### [Character level language model](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model_final_v3b.ipynb)
Built a model that generated new dinosaur names using a RNN model that could learn different name patterns from a previously collected dataset. The project focused on:
- How to store text data for processing using an RNN
- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit
- How to build a character-level text generation recurrent neural network
- Why clipping the gradients is important

### [Improvise a Jazz Solo with an LSTM Network](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v3a.ipynb)
Applied an LSTM to music generation and generate my own jazz music with deep learning.

### [Operations on word vectors](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb)
The project focused on:

- Loading pre-trained word vectors, and measure similarity using cosine similarity
- Using word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
- Modifying word embeddings to reduce their gender bias

### [Emojify](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%202/Emojify/Emojify_v2a.ipynb)
Used word vectors and an LSTM model to create an emojifier that could do the following:
- The model can turn the following sentence "Congratulations on the promotion! Let's get coffee and talk. Love you!" into "Congratulations on the promotion! 👍 Let's get coffee and talk. ☕️ Love you! ❤️"

### [Neural Machine Translation](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb)
Built a Neural Machine Translation (NMT) model to translate human-readable dates ("25th of June, 2009") into machine-readable dates ("2009-06-25").These, using an attention model, one of the most sophisticated sequence-to-sequence models.

### [Trigger Word Detection](https://github.com/Omar-Martinez/Deep-Learning-Specialization/blob/master/4.%20Sequence%20Models%20Course%20/Week%203/Trigger%20word%20detection/Trigger_word_detection_v1a.ipynb)
Learned about applying deep learning to speech recognition while constructing a speech dataset and implementing an algorithm for triggering word detection. By the end of this assignment, I was able to record a clip of myself talking, and have the algorithm trigger a chime when it detects the word "activate."

